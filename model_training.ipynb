{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 路径参数定义\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sys \n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def ensure_dir_exists(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    return dir\n",
    "\n",
    "data_dir = './origin_data'\n",
    "work_dir = './work'\n",
    "tagged_dataset_path = os.path.join(work_dir,'tagged_dataset.csv')\n",
    "ood_dataset_path = os.path.join(work_dir,'ood_dataset.csv')\n",
    "\n",
    "model_dir = ensure_dir_exists(os.path.join(work_dir, 'Model'))\n",
    "knn_model_path = os.path.join(model_dir, 'KNN.pickel')\n",
    "ood_detector_path = os.path.join(model_dir, 'Maha.pickel')\n",
    "\n",
    "dataset_dir = ensure_dir_exists(os.path.join(work_dir, 'Dataset'))\n",
    "test_data_path = os.path.join(dataset_dir, 'test_data.csv')\n",
    "test_label_path = os.path.join(dataset_dir, 'test_label.csv')\n",
    "train_data_path = os.path.join(dataset_dir, 'train_data.csv')\n",
    "train_label_path = os.path.join(dataset_dir, 'train_label.csv')\n",
    "\n",
    "ood_test_data_path = os.path.join(dataset_dir, 'ood_test_data.csv')\n",
    "ood_test_label_path = os.path.join(dataset_dir, 'ood_test_label.csv')\n",
    "ood_train_data_path = os.path.join(dataset_dir, 'ood_train_data.csv')\n",
    "ood_train_label_path = os.path.join(dataset_dir, 'ood_train_label.csv')\n",
    "\n",
    "clean_labeled_path = os.path.join(dataset_dir, 'clean_labeled.csv')\n",
    "clean_labeled_ood_path = os.path.join(dataset_dir, 'clean_labeled_ood.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 简介：为数据集打tag\n",
    "# 详请：从数据集目录下读取所有.log文件，根据文件名在csv中加入label，保存至工作区目录(区分OOD与In-distribtuion数据)\n",
    "\n",
    "\n",
    "files = os.listdir(data_dir)\n",
    "tagged_data = []\n",
    "OOD_data = []\n",
    "tag_file = None\n",
    "for log_file in files:\n",
    "    if not log_file.endswith('.log'): # 跳过csv字段文件\n",
    "        if log_file.endswith('.tag'):\n",
    "            tag_file = log_file\n",
    "        continue\n",
    "    label = log_file[0:log_file.rfind('_')] # 读取文件名\n",
    "    csv_rows = csv.reader(open(os.path.join(data_dir,log_file),'r'))\n",
    "    for csv_row in csv_rows:\n",
    "        csv_row.append(label)\n",
    "        if label == 'OOD': \n",
    "            OOD_data.append(csv_row)\n",
    "        else:\n",
    "            tagged_data.append(csv_row)\n",
    "            \n",
    "with open(tagged_dataset_path,  'w+') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerows(tagged_data)\n",
    "\n",
    "with open(ood_dataset_path,  'w+') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerows(OOD_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 543 entries, 0 to 542\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   timestamp             543 non-null    int64  \n",
      " 1   server_ip             543 non-null    object \n",
      " 2   server_port           543 non-null    int64  \n",
      " 3   client_ip             543 non-null    object \n",
      " 4   client_port           543 non-null    int64  \n",
      " 5   thread_ID             543 non-null    int64  \n",
      " 6   interval_sec          543 non-null    object \n",
      " 7   transfer_bits         543 non-null    int64  \n",
      " 8   bandwidth_bits        543 non-null    int64  \n",
      " 9   jitter_ms             543 non-null    float64\n",
      " 10  cnt_missingdatagrams  543 non-null    int64  \n",
      " 11  cnt_datagrams         543 non-null    int64  \n",
      " 12  cnt_errorrate         543 non-null    float64\n",
      " 13  cnt_outoforders       543 non-null    int64  \n",
      " 14  label                 543 non-null    object \n",
      "dtypes: float64(2), int64(9), object(4)\n",
      "memory usage: 63.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# 简介：读取tagged数据\n",
    "# 详情：读取tag文件里的csv字段，添上status后读取csv数据\n",
    "\n",
    "tag_file = os.path.join(data_dir, tag_file)\n",
    "\n",
    "col_names = next(csv.reader(open(tag_file, 'r')))# 读取tag\n",
    "col_names.append('label')\n",
    "\n",
    "data = pd.read_csv(tagged_dataset_path, header=None, names=col_names)\n",
    "data.info()\n",
    "ood_data = pd.read_csv(ood_dataset_path, header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 简介：数据预处理\n",
    "# 详情：删除部分无关数据项，对数据集进行划分并保存\n",
    "\n",
    "def delete_column(data, cols):\n",
    "    for col in cols:\n",
    "        data = data.drop(col, axis=1)\n",
    "    return data\n",
    "\n",
    "def save_if_not_exists(data, file_path):\n",
    "    exists = os.path.exists(file_path)\n",
    "    if not exists:\n",
    "        data.to_csv(file_path,index=False)\n",
    "    return exists\n",
    "\n",
    "label = pd.DataFrame(data['label'])\n",
    "data = delete_column(data, ['timestamp', 'server_ip', 'server_port', 'client_ip', 'client_port', 'thread_ID', 'label', 'interval_sec'])# 删除无关数据列\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2)\n",
    "\n",
    "label = pd.DataFrame(ood_data['label'])\n",
    "ood_data = delete_column(ood_data,['timestamp', 'server_ip', 'server_port', 'client_ip', 'client_port', 'thread_ID', 'label', 'interval_sec'])\n",
    "ood_X_train, ood_X_test, ood_y_train, ood_y_test = train_test_split(ood_data, label, test_size=0.2)\n",
    "\n",
    "# 保存数测试集、训练集\n",
    "# 避免Notebook运行全部cell时重复保存\n",
    "save_if_not_exists(data, clean_labeled_path)\n",
    "save_if_not_exists(ood_data, clean_labeled_ood_path)\n",
    "save_if_not_exists(X_train, train_data_path)\n",
    "save_if_not_exists(y_train, train_label_path)\n",
    "save_if_not_exists(X_test, test_data_path)\n",
    "save_if_not_exists(y_test, test_label_path)\n",
    "\n",
    "save_if_not_exists(ood_X_train, ood_train_data_path)\n",
    "save_if_not_exists(ood_y_train, ood_train_label_path)\n",
    "save_if_not_exists(ood_X_test, ood_test_data_path)\n",
    "save_if_not_exists(ood_y_test, ood_test_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 简介：KNN模型训练\n",
    "# 详情：读取训练集，训练，保存模型\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "x = pd.read_csv(train_data_path)\n",
    "y = pd.read_csv(train_label_path)\n",
    "y = y.values.ravel()\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=6)\n",
    "model.fit(x, y)\n",
    "\n",
    "with open(knn_model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix =\n",
      " [[ 9  0  1  0  0  0]\n",
      " [ 0 66  0  0  0  0]\n",
      " [ 0  0  8  0  0  0]\n",
      " [ 0  0  0  9  0  0]\n",
      " [ 0  0  3  0  2  0]\n",
      " [ 0  0  0  0  0 11]]\n",
      "Classification Report =\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         BW_ERROR       1.00      0.90      0.95        10\n",
      "CONCURRENCY_ERROR       1.00      1.00      1.00        66\n",
      "       DISK_ERROR       0.67      1.00      0.80         8\n",
      "    NETWORK_ERROR       1.00      1.00      1.00         9\n",
      "           NORMAL       1.00      0.40      0.57         5\n",
      "     SERVER_ERROR       1.00      1.00      1.00        11\n",
      "\n",
      "         accuracy                           0.96       109\n",
      "        macro avg       0.94      0.88      0.89       109\n",
      "     weighted avg       0.98      0.96      0.96       109\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 简介：KNN模型测试\n",
    "# 详情：读取测试集，测试模型，输出性能报告\n",
    "model = None\n",
    "with open(knn_model_path, 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "X_test = pd.read_csv(test_data_path)\n",
    "y_test = pd.read_csv(test_label_path)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix =\\n\", metrics.confusion_matrix(y_test, y_pred, labels=None, \n",
    "                                              sample_weight=None))\n",
    "\n",
    "print(\"Classification Report =\\n\", metrics.classification_report(y_test, y_pred, \n",
    "                                                                 labels=None, \n",
    "                                                                 target_names=None, \n",
    "                                                                 sample_weight=None, \n",
    "                                                                 digits=2, \n",
    "                                                                 output_dict=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix =\n",
      " [[88 21]\n",
      " [ 1 59]]\n",
      "Classification Report =\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.81      0.89       109\n",
      "           1       0.74      0.98      0.84        60\n",
      "\n",
      "    accuracy                           0.87       169\n",
      "   macro avg       0.86      0.90      0.87       169\n",
      "weighted avg       0.90      0.87      0.87       169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 简介：Mahalanobis OOD探测器\n",
    "# 详情：训练模型并测试，保存模型\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "class MahaDetector:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    def __init__(self):\n",
    "        self.threshold = None\n",
    "\n",
    "    def __data_partition(self, train_data_path, train_label_path):\n",
    "        X_train = pd.read_csv(train_data_path).values\n",
    "        y_train = pd.read_csv(train_label_path).values\n",
    "        label_dict = dict()\n",
    "        X = list()\n",
    "        for i in range(0,6):\n",
    "            X.append(np.zeros((0,7)))\n",
    "\n",
    "        for i in range(0, X_train.shape[0]):\n",
    "            _label = str(y_train[i][0])\n",
    "            if _label not in label_dict:\n",
    "                label_dict[_label] = len(label_dict)\n",
    "            element = X_train[i]\n",
    "            element = element.reshape(1,7)\n",
    "            X[label_dict[_label]] = np.append(X[label_dict[_label]], element, axis=0)\n",
    "        return X\n",
    "\n",
    "    def __get_mahalanobis_dis(self, x, vec):\n",
    "        xT = x.T  # 求转置\n",
    "        D = np.cov(xT)  # 求协方差矩阵\n",
    "        invD = np.linalg.inv(D)  # 协方差逆矩阵\n",
    "        x_A = vec\n",
    "        x_B = x.mean(axis=0)\n",
    "        tp = x_A - x_B\n",
    "        return np.sqrt(np.dot(np.dot(tp, invD), tp.T))[0]\n",
    "\n",
    "    def __get_min_maha_dis(self, X, vec):\n",
    "        vec = vec.reshape(1,7)\n",
    "        min_dis = None\n",
    "        for i in range(0,len(X)):\n",
    "            try:\n",
    "                if min_dis is None:\n",
    "                    min_dis = self.__get_mahalanobis_dis(X[0], vec)\n",
    "                else:\n",
    "                    tmp = min(self.__get_mahalanobis_dis(X[0], vec), min_dis)\n",
    "                    min_dis = min_dis if tmp is None else min(min_dis,tmp)\n",
    "            except:\n",
    "                pass\n",
    "        return float(min_dis)\n",
    "    \n",
    "    def __get_threshold(self, limit, false_list, true_list):\n",
    "        step = 0.01\n",
    "        threshold = step\n",
    "        cur_acc = 0\n",
    "        false_list_size = len(false_list)\n",
    "        true_list_size = len(true_list)\n",
    "        res = 0\n",
    "        while threshold < limit:\n",
    "            try:\n",
    "                if get_tpr(threshold, false_list, true_list) > 0.95 and get_acc(threshold, false_list, true_list) >= cur_acc:\n",
    "                    res = threshold\n",
    "            except:\n",
    "                pass\n",
    "            threshold += step\n",
    "        return res\n",
    "\n",
    "    def train(self, train_data_path, train_label_path, ood_train_data_path):\n",
    "        self.partitioned_data = self.__data_partition(train_data_path, train_label_path)\n",
    "        ood_X_train = pd.read_csv(ood_train_data_path).values\n",
    "        X_train = pd.read_csv(train_data_path).values\n",
    "        ood_maha_dis = list()\n",
    "        maha_dis = list()\n",
    "        for i in range(0, ood_X_train.shape[0]):\n",
    "            ood_maha_dis.append(self.__get_min_maha_dis(self.partitioned_data, ood_X_train[i]))\n",
    "        for i in range(0, X_train.shape[0]):\n",
    "            maha_dis.append(self.__get_min_maha_dis(self.partitioned_data, X_train[i]))\n",
    "\n",
    "        self.threshold = self.__get_threshold(min(ood_maha_dis)*1.2, ood_maha_dis, maha_dis)\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        if X is OOD data return True\n",
    "        '''\n",
    "        res = []\n",
    "        for x in X:\n",
    "            maha_dis = self.__get_min_maha_dis(self.partitioned_data, x)\n",
    "            is_ood = (maha_dis >= self.threshold)\n",
    "            res.append(is_ood)\n",
    "        return res\n",
    "\n",
    "def get_tnr(threshold, false_list, true_list):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    \n",
    "    for false_element in false_list:\n",
    "\n",
    "        if false_element >= threshold:\n",
    "            true_negative += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "    for true_element in true_list:\n",
    "        if true_element < threshold:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "    return (true_negative) / (true_negative + false_positive)\n",
    "\n",
    "def get_tpr(threshold, false_list, true_list):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    \n",
    "    for false_element in false_list:\n",
    "\n",
    "        if false_element >= threshold:\n",
    "            true_negative += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "    for true_element in true_list:\n",
    "        if true_element < threshold:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "    return (true_positive) / (true_positive + false_negative)\n",
    "\n",
    "def get_acc(threshold, false_list, true_list):\n",
    "    true_positive = 0\n",
    "    true_negative = 0\n",
    "    false_negative = 0\n",
    "    false_positive = 0\n",
    "    \n",
    "    for false_element in false_list:\n",
    "        if false_element >= threshold:\n",
    "            true_negative += 1\n",
    "        else:\n",
    "            false_negative += 1\n",
    "\n",
    "    for true_element in true_list:\n",
    "        if true_element < threshold:\n",
    "            true_positive += 1\n",
    "        else:\n",
    "            false_positive += 1\n",
    "    return (true_positive+true_negative) / (true_positive + false_negative + true_negative + false_positive)\n",
    "\n",
    "def get_auc(labels,probs):\n",
    "    ###initialize\n",
    "    P = 0\n",
    "    N = 0\n",
    "    for i in labels:\n",
    "        if (i == 1):\n",
    "            P += 1\n",
    "        else:\n",
    "            N += 1\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TPR_last = 0\n",
    "    FPR_last = 0\n",
    "    AUC = 0\n",
    "    pair = zip(probs, labels)\n",
    "    pair = sorted(pair, key=lambda x:x[0], reverse=True)\n",
    "    i = 0\n",
    "    while i < len(pair):\n",
    "        if (pair[i][1] == 1):\n",
    "            TP += 1\n",
    "        else:\n",
    "            FP += 1\n",
    "        ### maybe have the same probs\n",
    "        while (i + 1 < len(pair) and pair[i][0] == pair[i+1][0]):\n",
    "            i += 1\n",
    "            if (pair[i][1] == 1):\n",
    "                TP += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "        TPR = TP / P\n",
    "        FPR = FP / N\n",
    "        AUC += 0.5 * (TPR + TPR_last) * (FPR - FPR_last)\n",
    "        TPR_last = TPR\n",
    "        FPR_last = FPR\n",
    "        i += 1\n",
    "    return AUC\n",
    "\n",
    "\n",
    "\n",
    "MahaD = MahaDetector()\n",
    "MahaD.train(train_data_path, train_label_path, ood_train_data_path)\n",
    "\n",
    "# test detector\n",
    "ood_X_test = pd.read_csv(ood_test_data_path).values\n",
    "X_test = pd.read_csv(test_data_path).values\n",
    "\n",
    "labels = []\n",
    "preds = []\n",
    "for i in range(0, ood_X_test.shape[0]):\n",
    "    labels.append(1)\n",
    "preds = preds + (MahaD.predict(ood_X_test))\n",
    "for i in range(0, X_test.shape[0]):\n",
    "    labels.append(0)\n",
    "preds = preds + (MahaD.predict(X_test))\n",
    "\n",
    "# metrics\n",
    "\n",
    "print(\"Confusion Matrix =\\n\", metrics.confusion_matrix(labels, preds))\n",
    "\n",
    "print(\"Classification Report =\\n\", metrics.classification_report(labels, preds, \n",
    "                                                                 labels=None, \n",
    "                                                                 target_names=None, \n",
    "                                                                 sample_weight=None, \n",
    "                                                                 digits=2, \n",
    "                                                                 output_dict=False))\n",
    "\n",
    "\n",
    "with open(ood_detector_path,'wb') as f:\n",
    "    #f.write( pickle.dumps(info) )\n",
    "    pickle.dump(MahaD,f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
